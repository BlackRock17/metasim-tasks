"""
FastAPI server as middleware to LLM API endpoint.

This file creates a REST API server with two endpoints:
1. /clean-text - for cleaning text from artifacts
2. /chat - for generating responses in conversation

"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from typing import Dict, Any
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastAPI application
app = FastAPI(
    title="LLM Middleware API",
    description="FastAPI server as middleware to LLM for text cleaning and chat functionality",
    version="1.0.0"
)

# CORS middleware - allows requests from frontend applications
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production this should be more restrictive
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root() -> Dict[str, str]:
    """
    Root endpoint - returns API information.
    Used for health check and debugging.
    """
    return {
        "message": "LLM Middleware API is running",
        "version": "1.0.0",
        "endpoints": ["/clean-text", "/chat", "/docs"]
    }


@app.post("/clean-text")
async def clean_text(request: Dict[str, Any]) -> Dict[str, Any]:
    """
    Endpoint for cleaning text from artifacts.

    Accepts text and passes it to LLM Service for processing.
    Removes headers, footers, page numbers, etc.

    Args:
        request: Dict with "text" key containing text to clean

    Returns:
        Dict with "cleaned_text" key containing cleaned text
    """
    try:
        # Extract text from request
        if "text" not in request:
            raise HTTPException(status_code=400, detail="Missing 'text' field in request")

        input_text = request["text"]

        if not input_text or not isinstance(input_text, str):
            raise HTTPException(status_code=400, detail="'text' field must be non-empty string")

        logger.info(f"Received text cleaning request. Text length: {len(input_text)} characters")

        # TODO: Here we will integrate LLM Service
        # For now return dummy response for testing
        cleaned_text = f"[CLEANED] {input_text}"

        logger.info(f"Text cleaning completed. Result length: {len(cleaned_text)} characters")

        return {
            "cleaned_text": cleaned_text,
            "original_length": len(input_text),
            "cleaned_length": len(cleaned_text)
        }

    except HTTPException:
        # Re-raise HTTP exceptions
        raise
    except Exception as e:
        logger.error(f"Error in clean_text endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error during text cleaning")


@app.post("/chat")
async def chat(request: Dict[str, Any]) -> Dict[str, Any]:
    """
    Endpoint for generating response in conversation (chat).

    Accepts conversation history including new user message
    and returns response generated by LLM.

    Args:
        request: Dict with keys:
            - "messages": List of messages [{"role": "user/assistant", "content": "text"}]
            - "user_message": New message from user (optional)

    Returns:
        Dict with "response" key containing LLM response
    """
    try:
        # Validate input data
        if "messages" not in request and "user_message" not in request:
            raise HTTPException(
                status_code=400,
                detail="Missing 'messages' or 'user_message' field in request"
            )

        # Extract messages
        messages = request.get("messages", [])
        user_message = request.get("user_message", "")

        # If there's a new message, add it to history
        if user_message:
            messages.append({"role": "user", "content": user_message})

        if not messages:
            raise HTTPException(status_code=400, detail="No messages provided")

        logger.info(f"Received chat request with {len(messages)} messages")

        # TODO: Here we will integrate LLM Service for chat
        # For now return dummy response for testing
        last_user_message = messages[-1]["content"] if messages else ""
        dummy_response = f"As a buyer, I'm not sure about '{last_user_message}'. Can you tell me more about the benefits?"

        logger.info("Chat response generated successfully")

        return {
            "response": dummy_response,
            "message_count": len(messages),
            "role": "assistant"
        }

    except HTTPException:
        # Re-raise HTTP exceptions
        raise
    except Exception as e:
        logger.error(f"Error in chat endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error during chat processing")


@app.get("/health")
async def health_check() -> Dict[str, str]:
    """
    Health check endpoint - verifies server is working properly.
    """
    return {"status": "healthy", "service": "LLM Middleware"}


if __name__ == "__main__":
    # Start server when file is executed directly
    print("ğŸš€ Starting LLM Middleware API...")
    print("ğŸ“ Endpoints available:")
    print("   - http://localhost:8000/docs (Swagger UI)")
    print("   - POST http://localhost:8000/clean-text")
    print("   - POST http://localhost:8000/chat")

    uvicorn.run(
        "main:app",  # module:application
        host="0.0.0.0",  # Allow access from all IP addresses
        port=8000,  # Port where server runs
        reload=True  # Auto-restart on code changes (for development)
    )
